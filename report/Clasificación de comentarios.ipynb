{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ProgramData': virtualenv)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Víctor Daniel Cruz González"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Objetivo\n",
    "Establecer un ambiente de comunicación seguro y de calidad para los usuarios\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Antecedentes\n",
    "\n",
    "## Recurrent Neuronal Network\n",
    "Una red neuronal recurrente (RNN) es un tipo de red neuronal que utiliza datos secuenciales o series de tiempo. Estos algoritmos de aprendizaje profundo se utilizan para problemas temporales, como la traducción de idiomas, el procesamiento del lenguaje natural (nlp) o el reconocimiento de voz.\n",
    "\n",
    "![Unrolled network](unrolled-network.png)\n",
    "\n",
    "Mientras que las redes neuronales profundas asumen que las entradas y salidas son independientes entre sí, la salida de las recurrentes depende de los elementos anteriores. Asimismo, comparten parámetros en cada capa de la red. Las redes neuronales recurrentes comparten el mismo parámetro de peso dentro de cada capa de la red. Estos pesos todavía se ajustan en los procesos de retropropagación y descenso de gradiente para facilitar el aprendizaje por refuerzo.\n",
    "\n",
    "Las redes neuronales recurrentes aprovechan el algoritmo de retropropagación a través del tiempo (BPTT) para determinar los gradientes, que es ligeramente diferente de la retropropagación tradicional, ya que es específico de los datos de secuencia. Este algoritmo implica que el modelo se entrena a sí mismo calculando errores desde su capa de salida hasta su capa de entrada. Estos cálculos nos permiten ajustar los parámetros del modelo de manera adecuada. BPTT se diferencia del enfoque tradicional en que BPTT suma los errores en cada paso de tiempo, mientras que las redes de retroalimentación no necesitan sumar errores, ya que no comparten parámetros en cada capa.\n",
    "\n",
    "![BPPT](bptt.svg)\n",
    "\n",
    "A través de este proceso, los RNN tienden a encontrarse con dos problemas, conocidos como gradientes explosivos y gradientes que desaparecen. Estos problemas se definen por el tamaño del gradiente, que es la pendiente de la función de pérdida a lo largo de la curva de error. Cuando el gradiente es demasiado pequeño, continúa haciéndolo más pequeño, actualizando los parámetros de peso hasta que se vuelven insignificantes, es decir. 0. Cuando eso ocurre, el algoritmo ya no está aprendiendo. Los degradados explosivos se producen cuando el degradado es demasiado grande, creando un modelo inestable. En este caso, los pesos del modelo crecerán demasiado y, finalmente, se representarán como NaN. Una solución a estos problemas es reducir la cantidad de capas ocultas dentro de la red neuronal, eliminando parte de la complejidad del modelo RNN."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Implementación"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Inicializando librerias."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotGraphs(history, metric):\n",
    "    \"\"\"\n",
    "    Display graph of history and metric\n",
    "    \"\"\"\n",
    "    plt.plot(history.history[metric])\n",
    "    plt.plot(history.history['val_' + metric], '')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val' + metric])"
   ]
  },
  {
   "source": [
    "Obteniendo dataset de IMDB"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))\n"
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load(\n",
    "    'imdb_reviews', with_info=True, as_supervised=True)\n",
    "trainDataset, testDataset = dataset['train'], dataset['test']\n",
    "print(trainDataset.element_spec)"
   ]
  },
  {
   "source": [
    "Generando ejemplo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "text:  b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
      "label:  0\n"
     ]
    }
   ],
   "source": [
    "for example, label in trainDataset.take(1):\n",
    "    print('text: ', example.numpy())\n",
    "    print('label: ', label.numpy())"
   ]
  },
  {
   "source": [
    "Mezclando los datos"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "texts:  [b\"This piece of crap, since I can't call it a movie, can be summed up by the following.<br /><br />-Stereotypical black criminal with black midget partner get in trouble -Black Midget pretends to be a baby with a fully developed adult face, body hair and genitalia -Black midget is mistaken(somehow) by man and woman who happen to want a baby -Black midget than goes on to commit acts of physical and sexual violence, demean white people wherever he sees them, and commit more crimes -Happy Ending<br /><br />Honestly, it could have been a good satire if it hadn't been directed so shallowly and had such talentless bastards star in it.\"\n b'This movie blows - let\\'s get that straight right now. There are a few scene gems nestled inside this pile of crap but none can redeem the limp plot. Colin Farrel looks like Brad Pitt in \"12 Monkeys\" and acts in a similar manner. I normally hate Colin because he is a fairy in general but he\\'s OK in this movie. There were two plot lines in this movie-= one about a kid who throws rocks through windshields of moving vehicles and the other about a woman with a moustache. Let\\'s face it- this movie has no freaking idea of what it wanted to say or where it wanted to go. THe characters story lines intertwine on some levels but are in no means worthy of being included in a script. The whole thing is weak and pointless and then there is an occasional OK scene. But overall- Don\\'t bother unless you love irish accents so much that you can watch mediocrity and it is rescued by everyone sounding like the Lucky Charms elf -an American fetish that has catapulted some truly crappy movies to success.'\n b'I was so surprised when I saw this film so much underrated... I understand why some of you dislike this movie. Its pace is slow, a characteristic of Japanese films. Nevertheless, if you are absorbed in the film like me, you will find this not a problem at all.<br /><br />I must say this is the best comedy I have ever seen. \"Shall We Dansu?\" is often considered a masterpiece of Japanese comedies. It is very different from Hollywood ones, e.g. Austin Powers or Scary Movies, in which a gag is guaranteed in every couple of minutes. Rather, it is light-hearted, a movie that makes you feel good.<br /><br />I love the movie because it makes me feel \"real\". The plot is straightforward yet pleasing. I was so delighted seeing that Sugiyama (the main role) has found the meaning of life in dancing. Before I watched the film I was slightly depressed due to heavy schoolwork. I felt lost. However, this film made me think of the bright side of life. I believed I was in the same boat of Sugiyama; if he could find himself in his hobby, why couldn\\'t I? It reminded me of \"exploring my own future\" and discovering the happiness in my daily life.<br /><br />It is important to note that the actors are not professional dancers. While some of you may find the dancing scenes not as perfect as you expect, I kinda like it as it makes me feel that the characters are really \"alive\", learning to dance as the film goes on.<br /><br />Over all, this film is encouraging and heart-warming. As a comedy, it does its job perfectly. It definitely deserves 10 stars.<br /><br />And yes Aoki is funny :-D']\n\nlabels:  [0 0 1]\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "trainDataset = trainDataset.shuffle(BUFFER_SIZE).batch(\n",
    "    BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "testDataset = testDataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "for example, label in trainDataset.take(1):\n",
    "    print('texts: ', example.numpy()[:3])\n",
    "    print()\n",
    "    print('labels: ', label.numpy()[:3])"
   ]
  },
  {
   "source": [
    "Creando text encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['' '[UNK]' 'the' 'and' 'a' 'of' 'to' 'is' 'in' 'it' 'i' 'this' 'that'\n 'br' 'was' 'as' 'for' 'with' 'movie' 'but']\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(trainDataset.map(lambda text, label: text))\n",
    "vocab = np.array(encoder.get_vocabulary())\n",
    "print(vocab[:20])"
   ]
  },
  {
   "source": [
    "Mostrando ejemplos del encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[163  30   2 ...   0   0   0]\n [ 10 208  11 ...   0   0   0]\n [  2  61 274 ...   0   0   0]]\nOriginal:  b\"This movie is god awful. Not one quality to this movie. You would think that the gore would be good but it sucks bad. The effects are worse and the acting if you can call it acting is the worst I've ever seen. This movie was obviously shot on a camcorder and runs on a budget around 500 dollars probably. If you want to watch a good Zombie movie than watch Dawn of the dead or Day of the dead. If you want to watch a good cheap shot on video Zombie movie like this but way better than watch Redneck Zombies. Please avoid this movie at all costs. It is unwatchable and pointless. You've been warned. I've got nothing else to say about this stupid movie.\"\nRound-trip:  this movie is god awful not one quality to this movie you would think that the gore would be good but it [UNK] bad the effects are worse and the acting if you can call it acting is the worst ive ever seen this movie was obviously shot on a [UNK] and [UNK] on a budget around [UNK] [UNK] probably if you want to watch a good zombie movie than watch [UNK] of the dead or day of the dead if you want to watch a good cheap shot on video zombie movie like this but way better than watch [UNK] [UNK] please avoid this movie at all [UNK] it is [UNK] and [UNK] youve been [UNK] ive got nothing else to say about this stupid movie                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n\nOriginal:  b'My choice for greatest movie ever used to be Laughton\\'s \"Night of the Hunter\" which remains superb in my canon. But, it may have been supplanted by \"Shower\" which is the most artistically Daoist movie I have seen. The way that caring for others is represented by the flowing of water, and the way that water can be made inspiration, and comfort, and cleansing, and etc. is the essence of the Dao. It is possible to argue that the the NOFTH and Shower themes are similar, and that Lillian Gish in the former represents the purest form of Christianity as the operators of the bathhouse represent the purest form of Daoism. I would not in any way argue against such an interpretation. Both movies are visual joys in their integration of idea and image. Yet, Shower presents such an unstylized view of the sacredness of everyday life that I give it the nod. I revere both.'\nRound-trip:  my [UNK] for greatest movie ever used to be [UNK] night of the [UNK] which [UNK] superb in my [UNK] but it may have been [UNK] by [UNK] which is the most [UNK] [UNK] movie i have seen the way that [UNK] for others is [UNK] by the [UNK] of [UNK] and the way that [UNK] can be made [UNK] and [UNK] and [UNK] and etc is the [UNK] of the [UNK] it is possible to [UNK] that the the [UNK] and [UNK] [UNK] are similar and that [UNK] [UNK] in the [UNK] [UNK] the [UNK] form of [UNK] as the [UNK] of the [UNK] [UNK] the [UNK] form of [UNK] i would not in any way [UNK] against such an [UNK] both movies are [UNK] [UNK] in their [UNK] of idea and [UNK] yet [UNK] [UNK] such an [UNK] view of the [UNK] of [UNK] life that i give it the [UNK] i [UNK] both                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n\nOriginal:  b'I am a fairly big fan of most of the films that have been based on Stephen King\\'s books - this one rates as one of the scariest and most memorable.<br /><br />I have just finished rewatching it for about the tenth time and I still find it heart-wrenching as well as scary.<br /><br />The scene where Gage is on a sure collision course with the monster truck is one which stands out. And the \"No fair\" uttered by little Miko Hughes near the end is a touch of brilliance.<br /><br />'\nRound-trip:  i am a fairly big fan of most of the films that have been based on [UNK] [UNK] [UNK] this one [UNK] as one of the [UNK] and most [UNK] br i have just [UNK] [UNK] it for about the [UNK] time and i still find it [UNK] as well as [UNK] br the scene where [UNK] is on a sure [UNK] course with the monster [UNK] is one which [UNK] out and the no [UNK] [UNK] by little [UNK] [UNK] near the end is a [UNK] of [UNK] br                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n\n"
     ]
    }
   ],
   "source": [
    "for example, label in trainDataset.take(1):\r\n",
    "    encodedExample = encoder(example)[:3].numpy()\r\n",
    "    print(encodedExample)\r\n",
    "\r\n",
    "for example, label in trainDataset.take(1):\r\n",
    "    encodedExample = encoder(example)[:3].numpy()\r\n",
    "    for n in range(3):\r\n",
    "        print('Original: ', example[n].numpy())\r\n",
    "        print('Round-trip: ', ' '.join(vocab[encodedExample[n]]))\r\n",
    "        print()"
   ]
  },
  {
   "source": [
    "Implementando arquitectura para RNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[False, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        mask_zero=True\n",
    "    ),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1),\n",
    "])\n",
    "\n",
    "print([layer.supports_masking for layer in model.layers])\n"
   ]
  },
  {
   "source": [
    "Viendo si funciona la solución"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Texto sin padding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00603663], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "sampleText = ('The movie was cool. The animation and the graphics '\n",
    "              'were out of this world. I would recommend this movie.')\n",
    "predictions = model.predict(np.array([sampleText]))\n",
    "predictions[0]"
   ]
  },
  {
   "source": [
    "Texto con padding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00603663], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "padding = \"the \" * 2000\n",
    "predictions = model.predict(np.array([sampleText, padding]))\n",
    "predictions[0]"
   ]
  },
  {
   "source": [
    "Compilando la solución"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ntext_vectorization (TextVect (None, None)              0         \n_________________________________________________________________\nembedding (Embedding)        (None, None, 64)          64000     \n_________________________________________________________________\nbidirectional (Bidirectional (None, 128)               66048     \n_________________________________________________________________\ndense (Dense)                (None, 64)                8256      \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 138,369\nTrainable params: 138,369\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "Entrenando modelo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      " 52/391 [==>...........................] - ETA: 28:58 - loss: 0.6931 - accuracy: 0.5117"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-278fd325dd2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(trainDataset, epochs=10,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     validation_data=testDataset, validation_steps=30)\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtestLoss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestAcc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test Loss: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestLoss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(trainDataset, epochs=10,\n",
    "                    validation_data=testDataset, validation_steps=30)\n",
    "\n",
    "testLoss, testAcc = model.evaluate(testDataset)\n",
    "print('Test Loss: ', testLoss)\n",
    "print('Test Accuracy: ', testAcc)"
   ]
  },
  {
   "source": [
    "# Resultados"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plotGraphs(history, 'accuracy')\n",
    "plt.ylim(None, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plotGraphs(history, 'loss')\n",
    "plt.ylim(0, None)"
   ]
  },
  {
   "source": [
    "![](unrolled-network.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}